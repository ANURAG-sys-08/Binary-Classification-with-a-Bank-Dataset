{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12838929,"sourceType":"datasetVersion","datasetId":8120112}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Binary Classification with a Bank Dataset\nGoal is to predict whether a client will subscribe to a bank term deposit.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"`Evaluation`\n\nSubmissions are evaluated using ROC AUC between the predicted value and the observed target.\n\n`Dataset Description`\n\n`Files`\n\n*train.csv - the training dataset; y is the binary target\n\n*test.csv - the test dataset; your objective is to predict the probability y for each row\n\n*sample_submission.csv - a sample submission file in the correct format","metadata":{"execution":{"iopub.status.busy":"2025-08-22T17:49:21.553390Z","iopub.execute_input":"2025-08-22T17:49:21.553866Z","iopub.status.idle":"2025-08-22T17:49:21.565504Z","shell.execute_reply.started":"2025-08-22T17:49:21.553833Z","shell.execute_reply":"2025-08-22T17:49:21.564112Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:35.809589Z","iopub.execute_input":"2025-08-25T12:50:35.809915Z","iopub.status.idle":"2025-08-25T12:50:37.157481Z","shell.execute_reply.started":"2025-08-25T12:50:35.809890Z","shell.execute_reply":"2025-08-25T12:50:37.156501Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_csv = pd.read_csv(\"/kaggle/input/bank-dataset/train.csv\")\ntrain_csv.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:37.159449Z","iopub.execute_input":"2025-08-25T12:50:37.159920Z","iopub.status.idle":"2025-08-25T12:50:39.888094Z","shell.execute_reply.started":"2025-08-25T12:50:37.159882Z","shell.execute_reply":"2025-08-25T12:50:39.887068Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id  age          job  marital  education default  balance housing loan  \\\n0   0   42   technician  married  secondary      no        7      no   no   \n1   1   38  blue-collar  married  secondary      no      514      no   no   \n2   2   36  blue-collar  married  secondary      no      602     yes   no   \n3   3   27      student   single  secondary      no       34     yes   no   \n4   4   26   technician  married  secondary      no      889     yes   no   \n\n    contact  day month  duration  campaign  pdays  previous poutcome  y  \n0  cellular   25   aug       117         3     -1         0  unknown  0  \n1   unknown   18   jun       185         1     -1         0  unknown  0  \n2   unknown   14   may       111         2     -1         0  unknown  0  \n3   unknown   28   may        10         2     -1         0  unknown  0  \n4  cellular    3   feb       902         1     -1         0  unknown  1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>job</th>\n      <th>marital</th>\n      <th>education</th>\n      <th>default</th>\n      <th>balance</th>\n      <th>housing</th>\n      <th>loan</th>\n      <th>contact</th>\n      <th>day</th>\n      <th>month</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>poutcome</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>42</td>\n      <td>technician</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>7</td>\n      <td>no</td>\n      <td>no</td>\n      <td>cellular</td>\n      <td>25</td>\n      <td>aug</td>\n      <td>117</td>\n      <td>3</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>38</td>\n      <td>blue-collar</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>514</td>\n      <td>no</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>18</td>\n      <td>jun</td>\n      <td>185</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>36</td>\n      <td>blue-collar</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>602</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>14</td>\n      <td>may</td>\n      <td>111</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>27</td>\n      <td>student</td>\n      <td>single</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>34</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>unknown</td>\n      <td>28</td>\n      <td>may</td>\n      <td>10</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>26</td>\n      <td>technician</td>\n      <td>married</td>\n      <td>secondary</td>\n      <td>no</td>\n      <td>889</td>\n      <td>yes</td>\n      <td>no</td>\n      <td>cellular</td>\n      <td>3</td>\n      <td>feb</td>\n      <td>902</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"train_csv[\"y\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:39.889147Z","iopub.execute_input":"2025-08-25T12:50:39.889439Z","iopub.status.idle":"2025-08-25T12:50:39.908386Z","shell.execute_reply.started":"2025-08-25T12:50:39.889415Z","shell.execute_reply":"2025-08-25T12:50:39.907401Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"y\n0    659512\n1     90488\nName: count, dtype: int64"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"we can see that there is big value imbalance.","metadata":{}},{"cell_type":"code","source":"train_csv[\"job\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:39.910430Z","iopub.execute_input":"2025-08-25T12:50:39.910726Z","iopub.status.idle":"2025-08-25T12:50:39.991294Z","shell.execute_reply.started":"2025-08-25T12:50:39.910703Z","shell.execute_reply":"2025-08-25T12:50:39.990304Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"job\nmanagement       175541\nblue-collar      170498\ntechnician       138107\nadmin.            81492\nservices          64209\nretired           35185\nself-employed     19020\nentrepreneur      17718\nunemployed        17634\nhousemaid         15912\nstudent           11767\nunknown            2917\nName: count, dtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_csv[\"education\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:39.992247Z","iopub.execute_input":"2025-08-25T12:50:39.992634Z","iopub.status.idle":"2025-08-25T12:50:40.072999Z","shell.execute_reply.started":"2025-08-25T12:50:39.992590Z","shell.execute_reply":"2025-08-25T12:50:40.072163Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"education\nsecondary    401683\ntertiary     227508\nprimary       99510\nunknown       21299\nName: count, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_csv[\"default\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:40.073999Z","iopub.execute_input":"2025-08-25T12:50:40.074288Z","iopub.status.idle":"2025-08-25T12:50:40.154774Z","shell.execute_reply.started":"2025-08-25T12:50:40.074259Z","shell.execute_reply":"2025-08-25T12:50:40.153902Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"default\nno     737151\nyes     12849\nName: count, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_csv.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:40.155786Z","iopub.execute_input":"2025-08-25T12:50:40.156082Z","iopub.status.idle":"2025-08-25T12:50:40.411124Z","shell.execute_reply.started":"2025-08-25T12:50:40.156050Z","shell.execute_reply":"2025-08-25T12:50:40.409971Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                  id            age        balance            day  \\\ncount  750000.000000  750000.000000  750000.000000  750000.000000   \nmean   374999.500000      40.926395    1204.067397      16.117209   \nstd    216506.495284      10.098829    2836.096759       8.250832   \nmin         0.000000      18.000000   -8019.000000       1.000000   \n25%    187499.750000      33.000000       0.000000       9.000000   \n50%    374999.500000      39.000000     634.000000      17.000000   \n75%    562499.250000      48.000000    1390.000000      21.000000   \nmax    749999.000000      95.000000   99717.000000      31.000000   \n\n            duration       campaign          pdays       previous  \\\ncount  750000.000000  750000.000000  750000.000000  750000.000000   \nmean      256.229144       2.577008      22.412733       0.298545   \nstd       272.555662       2.718514      77.319998       1.335926   \nmin         1.000000       1.000000      -1.000000       0.000000   \n25%        91.000000       1.000000      -1.000000       0.000000   \n50%       133.000000       2.000000      -1.000000       0.000000   \n75%       361.000000       3.000000      -1.000000       0.000000   \nmax      4918.000000      63.000000     871.000000     200.000000   \n\n                   y  \ncount  750000.000000  \nmean        0.120651  \nstd         0.325721  \nmin         0.000000  \n25%         0.000000  \n50%         0.000000  \n75%         0.000000  \nmax         1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>balance</th>\n      <th>day</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>750000.000000</td>\n      <td>750000.000000</td>\n      <td>750000.000000</td>\n      <td>750000.000000</td>\n      <td>750000.000000</td>\n      <td>750000.000000</td>\n      <td>750000.000000</td>\n      <td>750000.000000</td>\n      <td>750000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>374999.500000</td>\n      <td>40.926395</td>\n      <td>1204.067397</td>\n      <td>16.117209</td>\n      <td>256.229144</td>\n      <td>2.577008</td>\n      <td>22.412733</td>\n      <td>0.298545</td>\n      <td>0.120651</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>216506.495284</td>\n      <td>10.098829</td>\n      <td>2836.096759</td>\n      <td>8.250832</td>\n      <td>272.555662</td>\n      <td>2.718514</td>\n      <td>77.319998</td>\n      <td>1.335926</td>\n      <td>0.325721</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>-8019.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>187499.750000</td>\n      <td>33.000000</td>\n      <td>0.000000</td>\n      <td>9.000000</td>\n      <td>91.000000</td>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>374999.500000</td>\n      <td>39.000000</td>\n      <td>634.000000</td>\n      <td>17.000000</td>\n      <td>133.000000</td>\n      <td>2.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>562499.250000</td>\n      <td>48.000000</td>\n      <td>1390.000000</td>\n      <td>21.000000</td>\n      <td>361.000000</td>\n      <td>3.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>749999.000000</td>\n      <td>95.000000</td>\n      <td>99717.000000</td>\n      <td>31.000000</td>\n      <td>4918.000000</td>\n      <td>63.000000</td>\n      <td>871.000000</td>\n      <td>200.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_csv=train_csv.drop(\"marital\",axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:40.412283Z","iopub.execute_input":"2025-08-25T12:50:40.412701Z","iopub.status.idle":"2025-08-25T12:50:40.494084Z","shell.execute_reply.started":"2025-08-25T12:50:40.412666Z","shell.execute_reply":"2025-08-25T12:50:40.493122Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_csv.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:40.495261Z","iopub.execute_input":"2025-08-25T12:50:40.495611Z","iopub.status.idle":"2025-08-25T12:50:40.826323Z","shell.execute_reply.started":"2025-08-25T12:50:40.495576Z","shell.execute_reply":"2025-08-25T12:50:40.825377Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 750000 entries, 0 to 749999\nData columns (total 17 columns):\n #   Column     Non-Null Count   Dtype \n---  ------     --------------   ----- \n 0   id         750000 non-null  int64 \n 1   age        750000 non-null  int64 \n 2   job        750000 non-null  object\n 3   education  750000 non-null  object\n 4   default    750000 non-null  object\n 5   balance    750000 non-null  int64 \n 6   housing    750000 non-null  object\n 7   loan       750000 non-null  object\n 8   contact    750000 non-null  object\n 9   day        750000 non-null  int64 \n 10  month      750000 non-null  object\n 11  duration   750000 non-null  int64 \n 12  campaign   750000 non-null  int64 \n 13  pdays      750000 non-null  int64 \n 14  previous   750000 non-null  int64 \n 15  poutcome   750000 non-null  object\n 16  y          750000 non-null  int64 \ndtypes: int64(9), object(8)\nmemory usage: 97.3+ MB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"for label, content in train_csv.items():\n    if pd.api.types.is_string_dtype(content):\n        print(f\"{label}\")\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:40.829380Z","iopub.execute_input":"2025-08-25T12:50:40.829717Z","iopub.status.idle":"2025-08-25T12:50:40.902526Z","shell.execute_reply.started":"2025-08-25T12:50:40.829692Z","shell.execute_reply":"2025-08-25T12:50:40.901220Z"}},"outputs":[{"name":"stdout","text":"job\neducation\ndefault\nhousing\nloan\ncontact\nmonth\npoutcome\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"train_csv = train_csv.drop(\"id\",axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:40.903403Z","iopub.execute_input":"2025-08-25T12:50:40.903696Z","iopub.status.idle":"2025-08-25T12:50:41.004637Z","shell.execute_reply.started":"2025-08-25T12:50:40.903675Z","shell.execute_reply":"2025-08-25T12:50:41.003526Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"binary_cols = [\"housing\",\"loan\",\"default\"]\n\nfor cols in binary_cols:\n    train_csv[cols] = train_csv[cols].map({'yes':1,'no':0})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:41.005539Z","iopub.execute_input":"2025-08-25T12:50:41.005847Z","iopub.status.idle":"2025-08-25T12:50:41.152277Z","shell.execute_reply.started":"2025-08-25T12:50:41.005815Z","shell.execute_reply":"2025-08-25T12:50:41.151316Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_csv.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:41.153414Z","iopub.execute_input":"2025-08-25T12:50:41.153715Z","iopub.status.idle":"2025-08-25T12:50:41.168452Z","shell.execute_reply.started":"2025-08-25T12:50:41.153694Z","shell.execute_reply":"2025-08-25T12:50:41.167041Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   age          job  education  default  balance  housing  loan   contact  \\\n0   42   technician  secondary        0        7        0     0  cellular   \n1   38  blue-collar  secondary        0      514        0     0   unknown   \n2   36  blue-collar  secondary        0      602        1     0   unknown   \n3   27      student  secondary        0       34        1     0   unknown   \n4   26   technician  secondary        0      889        1     0  cellular   \n\n   day month  duration  campaign  pdays  previous poutcome  y  \n0   25   aug       117         3     -1         0  unknown  0  \n1   18   jun       185         1     -1         0  unknown  0  \n2   14   may       111         2     -1         0  unknown  0  \n3   28   may        10         2     -1         0  unknown  0  \n4    3   feb       902         1     -1         0  unknown  1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>job</th>\n      <th>education</th>\n      <th>default</th>\n      <th>balance</th>\n      <th>housing</th>\n      <th>loan</th>\n      <th>contact</th>\n      <th>day</th>\n      <th>month</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>pdays</th>\n      <th>previous</th>\n      <th>poutcome</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>42</td>\n      <td>technician</td>\n      <td>secondary</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>cellular</td>\n      <td>25</td>\n      <td>aug</td>\n      <td>117</td>\n      <td>3</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>blue-collar</td>\n      <td>secondary</td>\n      <td>0</td>\n      <td>514</td>\n      <td>0</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>18</td>\n      <td>jun</td>\n      <td>185</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36</td>\n      <td>blue-collar</td>\n      <td>secondary</td>\n      <td>0</td>\n      <td>602</td>\n      <td>1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>14</td>\n      <td>may</td>\n      <td>111</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>student</td>\n      <td>secondary</td>\n      <td>0</td>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>28</td>\n      <td>may</td>\n      <td>10</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26</td>\n      <td>technician</td>\n      <td>secondary</td>\n      <td>0</td>\n      <td>889</td>\n      <td>1</td>\n      <td>0</td>\n      <td>cellular</td>\n      <td>3</td>\n      <td>feb</td>\n      <td>902</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>unknown</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"categorical_variables = [\"job\", \"education\", \"contact\", \"month\", \"poutcome\"]\n\ntrain_csv_encoded = pd.get_dummies(train_csv, columns = categorical_variables, drop_first = True,dtype=int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:41.170021Z","iopub.execute_input":"2025-08-25T12:50:41.170367Z","iopub.status.idle":"2025-08-25T12:50:41.892117Z","shell.execute_reply.started":"2025-08-25T12:50:41.170335Z","shell.execute_reply":"2025-08-25T12:50:41.890595Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_csv[\"pdays\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:41.893092Z","iopub.execute_input":"2025-08-25T12:50:41.893444Z","iopub.status.idle":"2025-08-25T12:50:41.907423Z","shell.execute_reply.started":"2025-08-25T12:50:41.893407Z","shell.execute_reply":"2025-08-25T12:50:41.906299Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"pdays\n-1      672434\n 182      2515\n 92       2275\n 183      2074\n 181      1698\n         ...  \n 759         1\n 529         1\n 794         1\n 617         1\n 376         1\nName: count, Length: 596, dtype: int64"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"train_csv_encoded['pcontacted'] = (train_csv_encoded['pdays'] != -1).astype(int)\ntrain_csv_encoded = train_csv_encoded.drop(\"pdays\",axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:41.908514Z","iopub.execute_input":"2025-08-25T12:50:41.908880Z","iopub.status.idle":"2025-08-25T12:50:42.014530Z","shell.execute_reply.started":"2025-08-25T12:50:41.908845Z","shell.execute_reply":"2025-08-25T12:50:42.013302Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_csv_encoded.head().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:42.015601Z","iopub.execute_input":"2025-08-25T12:50:42.015899Z","iopub.status.idle":"2025-08-25T12:50:42.029066Z","shell.execute_reply.started":"2025-08-25T12:50:42.015877Z","shell.execute_reply":"2025-08-25T12:50:42.027695Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                       0    1    2   3    4\nage                   42   38   36  27   26\ndefault                0    0    0   0    0\nbalance                7  514  602  34  889\nhousing                0    0    1   1    1\nloan                   0    0    0   0    0\nday                   25   18   14  28    3\nduration             117  185  111  10  902\ncampaign               3    1    2   2    1\nprevious               0    0    0   0    0\ny                      0    0    0   0    1\njob_blue-collar        0    1    1   0    0\njob_entrepreneur       0    0    0   0    0\njob_housemaid          0    0    0   0    0\njob_management         0    0    0   0    0\njob_retired            0    0    0   0    0\njob_self-employed      0    0    0   0    0\njob_services           0    0    0   0    0\njob_student            0    0    0   1    0\njob_technician         1    0    0   0    1\njob_unemployed         0    0    0   0    0\njob_unknown            0    0    0   0    0\neducation_secondary    1    1    1   1    1\neducation_tertiary     0    0    0   0    0\neducation_unknown      0    0    0   0    0\ncontact_telephone      0    0    0   0    0\ncontact_unknown        0    1    1   1    0\nmonth_aug              1    0    0   0    0\nmonth_dec              0    0    0   0    0\nmonth_feb              0    0    0   0    1\nmonth_jan              0    0    0   0    0\nmonth_jul              0    0    0   0    0\nmonth_jun              0    1    0   0    0\nmonth_mar              0    0    0   0    0\nmonth_may              0    0    1   1    0\nmonth_nov              0    0    0   0    0\nmonth_oct              0    0    0   0    0\nmonth_sep              0    0    0   0    0\npoutcome_other         0    0    0   0    0\npoutcome_success       0    0    0   0    0\npoutcome_unknown       1    1    1   1    1\npcontacted             0    0    0   0    0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>age</th>\n      <td>42</td>\n      <td>38</td>\n      <td>36</td>\n      <td>27</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>default</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>balance</th>\n      <td>7</td>\n      <td>514</td>\n      <td>602</td>\n      <td>34</td>\n      <td>889</td>\n    </tr>\n    <tr>\n      <th>housing</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>loan</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>day</th>\n      <td>25</td>\n      <td>18</td>\n      <td>14</td>\n      <td>28</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>duration</th>\n      <td>117</td>\n      <td>185</td>\n      <td>111</td>\n      <td>10</td>\n      <td>902</td>\n    </tr>\n    <tr>\n      <th>campaign</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>previous</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>y</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>job_blue-collar</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_entrepreneur</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_housemaid</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_management</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_retired</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_self-employed</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_services</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_student</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_technician</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>job_unemployed</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>job_unknown</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>education_secondary</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>education_tertiary</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>education_unknown</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>contact_telephone</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>contact_unknown</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_aug</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_dec</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_feb</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>month_jan</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_jul</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_jun</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_mar</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_may</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_nov</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_oct</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>month_sep</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>poutcome_other</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>poutcome_success</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>poutcome_unknown</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>pcontacted</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"x = train_csv_encoded.drop(\"y\",axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:42.030144Z","iopub.execute_input":"2025-08-25T12:50:42.030513Z","iopub.status.idle":"2025-08-25T12:50:42.137020Z","shell.execute_reply.started":"2025-08-25T12:50:42.030478Z","shell.execute_reply":"2025-08-25T12:50:42.135933Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"y = train_csv_encoded[\"y\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:42.138272Z","iopub.execute_input":"2025-08-25T12:50:42.138680Z","iopub.status.idle":"2025-08-25T12:50:42.144219Z","shell.execute_reply.started":"2025-08-25T12:50:42.138647Z","shell.execute_reply":"2025-08-25T12:50:42.143021Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"test_csv = pd.read_csv(\"/kaggle/input/bank-dataset/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:42.145284Z","iopub.execute_input":"2025-08-25T12:50:42.146190Z","iopub.status.idle":"2025-08-25T12:50:43.014914Z","shell.execute_reply.started":"2025-08-25T12:50:42.146148Z","shell.execute_reply":"2025-08-25T12:50:43.013968Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:43.016054Z","iopub.execute_input":"2025-08-25T12:50:43.016375Z","iopub.status.idle":"2025-08-25T12:50:43.199726Z","shell.execute_reply.started":"2025-08-25T12:50:43.016341Z","shell.execute_reply":"2025-08-25T12:50:43.198420Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2 , random_state=42,stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:43.200697Z","iopub.execute_input":"2025-08-25T12:50:43.201172Z","iopub.status.idle":"2025-08-25T12:50:43.873959Z","shell.execute_reply.started":"2025-08-25T12:50:43.201142Z","shell.execute_reply":"2025-08-25T12:50:43.873081Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def fit_and_score(model, x_train, y_train, x_test, y_test):\n    model.fit(x_train, y_train)\n    score = model.score(x_test, y_test)    \n    return score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:43.874918Z","iopub.execute_input":"2025-08-25T12:50:43.875153Z","iopub.status.idle":"2025-08-25T12:50:43.880995Z","shell.execute_reply.started":"2025-08-25T12:50:43.875134Z","shell.execute_reply":"2025-08-25T12:50:43.879996Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"#### LET'S TRAIN SOME MODELS ON OUR DATA\nI WILL USE Logistic Regression, XGBoost , LightGBM ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\nlog_reg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\nfit_and_score(log_reg, x_train_scaled, y_train, x_test_scaled, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:43.882062Z","iopub.execute_input":"2025-08-25T12:50:43.882327Z","iopub.status.idle":"2025-08-25T12:50:47.278885Z","shell.execute_reply.started":"2025-08-25T12:50:43.882305Z","shell.execute_reply":"2025-08-25T12:50:47.276533Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"0.8690666666666667"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# let's try XGBOOST\nfrom xgboost import XGBClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:47.280022Z","iopub.execute_input":"2025-08-25T12:50:47.280375Z","iopub.status.idle":"2025-08-25T12:50:47.601120Z","shell.execute_reply.started":"2025-08-25T12:50:47.280345Z","shell.execute_reply":"2025-08-25T12:50:47.600128Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\nxgb = XGBClassifier(\n    objective='binary:logistic',\n    eval_metric='auc',\n    scale_pos_weight=scale_pos_weight,\n    use_label_encoder=False,\n    random_state=42\n)\nfit_and_score(xgb,x_train, y_train, x_test, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:47.602341Z","iopub.execute_input":"2025-08-25T12:50:47.602703Z","iopub.status.idle":"2025-08-25T12:50:52.621859Z","shell.execute_reply.started":"2025-08-25T12:50:47.602674Z","shell.execute_reply":"2025-08-25T12:50:52.620943Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.88596"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# let's try lightbgm\nimport lightgbm as lgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:52.622818Z","iopub.execute_input":"2025-08-25T12:50:52.623140Z","iopub.status.idle":"2025-08-25T12:50:59.260539Z","shell.execute_reply.started":"2025-08-25T12:50:52.623117Z","shell.execute_reply":"2025-08-25T12:50:59.259433Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"lgbm = lgb.LGBMClassifier(\n    objective='binary',\n    metric='auc',\n    is_unbalance=True, # Simple parameter to handle imbalance\n    random_state=42\n)\nfit_and_score(lgbm,x_train, y_train, x_test, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:50:59.265352Z","iopub.execute_input":"2025-08-25T12:50:59.266002Z","iopub.status.idle":"2025-08-25T12:51:05.125944Z","shell.execute_reply.started":"2025-08-25T12:50:59.265974Z","shell.execute_reply":"2025-08-25T12:51:05.125113Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049311 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.8795266666666667"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"print(\"The score of logistic reagression is 0.86844\")\n\nprint(\"The score of xgboost is 0.88461\")\n\nprint(\"The score of lightbgm classifier is 0.87854\")","metadata":{}},{"cell_type":"markdown","source":"#### HYPERTUNING THE MODELS ","metadata":{}},{"cell_type":"code","source":"#parameters grid for hypertuning \nfrom scipy.stats import uniform\n\n# Parameter grid for Logistic Regression\nlog_reg_grid = {\n    'C': uniform(0.1, 20),\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear']\n}\n\n# Parameter grid for XGBoost\nxgb_grid = {\n    'n_estimators': [100, 200, 500, 1000],\n    'max_depth': [3, 5, 7, 10],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'subsample': [0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n}\n\n# Parameter grid for LightGBM\nlgbm_grid = {\n    'n_estimators': [100, 200, 500, 1000],\n    'num_leaves': [20, 31, 40, 50, 60],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'subsample': [0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:51:05.126959Z","iopub.execute_input":"2025-08-25T12:51:05.127194Z","iopub.status.idle":"2025-08-25T12:51:05.135480Z","shell.execute_reply.started":"2025-08-25T12:51:05.127176Z","shell.execute_reply":"2025-08-25T12:51:05.134180Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nlog_reg_1 = RandomizedSearchCV(log_reg,\n                              param_distributions = log_reg_grid,\n                              n_iter = 10,\n                              n_jobs = -1,\n                              verbose =True,\n                              random_state = 42,\n                              cv = 5,\n                              scoring = 'roc_auc')\nlog_reg_1.fit(x_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:52:47.944296Z","iopub.execute_input":"2025-08-25T12:52:47.944668Z","iopub.status.idle":"2025-08-25T12:55:48.934377Z","shell.execute_reply.started":"2025-08-25T12:52:47.944643Z","shell.execute_reply":"2025-08-25T12:55:48.933362Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=5,\n                   estimator=LogisticRegression(class_weight='balanced',\n                                                max_iter=1000,\n                                                random_state=42),\n                   n_jobs=-1,\n                   param_distributions={'C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7b28a0f5c810>,\n                                        'penalty': ['l1', 'l2'],\n                                        'solver': ['liblinear']},\n                   random_state=42, scoring='roc_auc', verbose=True)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n                                                max_iter=1000,\n                                                random_state=42),\n                   n_jobs=-1,\n                   param_distributions={&#x27;C&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7b28a0f5c810&gt;,\n                                        &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n                                        &#x27;solver&#x27;: [&#x27;liblinear&#x27;]},\n                   random_state=42, scoring=&#x27;roc_auc&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n                                                max_iter=1000,\n                                                random_state=42),\n                   n_jobs=-1,\n                   param_distributions={&#x27;C&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7b28a0f5c810&gt;,\n                                        &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;],\n                                        &#x27;solver&#x27;: [&#x27;liblinear&#x27;]},\n                   random_state=42, scoring=&#x27;roc_auc&#x27;, verbose=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"log_reg_1.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:55:57.376736Z","iopub.execute_input":"2025-08-25T12:55:57.377086Z","iopub.status.idle":"2025-08-25T12:55:57.383719Z","shell.execute_reply.started":"2025-08-25T12:55:57.377058Z","shell.execute_reply":"2025-08-25T12:55:57.382580Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"{'C': 12.073169683940732, 'penalty': 'l1', 'solver': 'liblinear'}"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"log_reg_1.score(x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:56:02.158058Z","iopub.execute_input":"2025-08-25T12:56:02.158448Z","iopub.status.idle":"2025-08-25T12:56:02.276251Z","shell.execute_reply.started":"2025-08-25T12:56:02.158418Z","shell.execute_reply":"2025-08-25T12:56:02.275298Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"0.9426579866416428"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nlog_reg_grid = {\n    'C': [12.073169683940732], \n    'penalty': ['l1'], \n    'solver': ['liblinear']\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:56:11.439416Z","iopub.execute_input":"2025-08-25T12:56:11.439943Z","iopub.status.idle":"2025-08-25T12:56:11.445463Z","shell.execute_reply.started":"2025-08-25T12:56:11.439915Z","shell.execute_reply":"2025-08-25T12:56:11.444416Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"log_reg_grid_model = GridSearchCV(estimator=log_reg,\n                                 param_grid = log_reg_grid,\n                                 n_jobs = -1,\n                                 verbose = 2,\n                                 scoring = 'roc_auc',\n                                 cv = 5)\nlog_reg_grid_model.fit(x_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:56:37.310334Z","iopub.execute_input":"2025-08-25T12:56:37.310717Z","iopub.status.idle":"2025-08-25T12:57:14.858106Z","shell.execute_reply.started":"2025-08-25T12:56:37.310687Z","shell.execute_reply":"2025-08-25T12:57:14.857048Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 1 candidates, totalling 5 fits\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(cv=5,\n             estimator=LogisticRegression(class_weight='balanced',\n                                          max_iter=1000, random_state=42),\n             n_jobs=-1,\n             param_grid={'C': [12.073169683940732], 'penalty': ['l1'],\n                         'solver': ['liblinear']},\n             scoring='roc_auc', verbose=2)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n             estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n                                          max_iter=1000, random_state=42),\n             n_jobs=-1,\n             param_grid={&#x27;C&#x27;: [12.073169683940732], &#x27;penalty&#x27;: [&#x27;l1&#x27;],\n                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;]},\n             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n             estimator=LogisticRegression(class_weight=&#x27;balanced&#x27;,\n                                          max_iter=1000, random_state=42),\n             n_jobs=-1,\n             param_grid={&#x27;C&#x27;: [12.073169683940732], &#x27;penalty&#x27;: [&#x27;l1&#x27;],\n                         &#x27;solver&#x27;: [&#x27;liblinear&#x27;]},\n             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"log_reg_grid_model.score(x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:57:31.963360Z","iopub.execute_input":"2025-08-25T12:57:31.963718Z","iopub.status.idle":"2025-08-25T12:57:32.087769Z","shell.execute_reply.started":"2025-08-25T12:57:31.963692Z","shell.execute_reply":"2025-08-25T12:57:32.086599Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0.9426579866416428"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"#lets hypertune xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T12:51:56.379599Z","iopub.status.idle":"2025-08-25T12:51:56.380058Z","shell.execute_reply.started":"2025-08-25T12:51:56.379845Z","shell.execute_reply":"2025-08-25T12:51:56.379866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_model_1 = RandomizedSearchCV(xgb,\n                            param_distributions = xgb_grid,\n                            n_iter = 20,\n                            random_state = 42,\n                            verbose = 2,\n                            n_jobs = -1,\n                            cv = 5,\n                            scoring = 'roc_auc',\n                            )\nxgb_model_1.fit(x_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:07:02.433427Z","iopub.execute_input":"2025-08-25T13:07:02.434009Z","iopub.status.idle":"2025-08-25T13:42:18.748626Z","shell.execute_reply.started":"2025-08-25T13:07:02.433981Z","shell.execute_reply":"2025-08-25T13:42:18.746500Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 20 candidates, totalling 100 fits\n[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.6min\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  33.0s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=0.7; total time= 3.5min\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.6min\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  33.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  32.9s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=0.7; total time= 3.5min\n[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.5min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=0.7; total time= 3.6min\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=1000, subsample=1.0; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=500, subsample=0.9; total time= 1.2min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=500, subsample=0.9; total time= 1.2min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=3, n_estimators=500, subsample=1.0; total time=  41.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=1.0; total time= 2.9min\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=1000, subsample=1.0; total time= 1.9min\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=1000, subsample=1.0; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=500, subsample=0.9; total time= 1.2min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=3, n_estimators=500, subsample=1.0; total time=  41.7s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=1.0; total time= 2.9min\n[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.6min\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  33.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  32.3s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=0.7; total time= 3.7min\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=1000, subsample=1.0; total time= 2.0min\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=5, n_estimators=1000, subsample=1.0; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=500, subsample=0.9; total time= 1.2min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=3, n_estimators=500, subsample=1.0; total time=  41.5s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=1.0; total time= 2.8min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=0.7; total time= 3.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=500, subsample=0.9; total time= 1.2min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=3, n_estimators=500, subsample=1.0; total time=  42.1s\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=3, n_estimators=500, subsample=1.0; total time=  42.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=1.0; total time= 2.9min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=10, n_estimators=1000, subsample=1.0; total time= 2.8min\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=1000, subsample=0.7; total time= 1.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=3, n_estimators=500, subsample=1.0; total time=  40.5s\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=3, n_estimators=500, subsample=1.0; total time=  41.5s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, n_estimators=100, subsample=0.9; total time=  25.1s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, n_estimators=100, subsample=0.9; total time=  23.8s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9; total time=  20.4s\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=  31.2s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  33.0s\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.2min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=5, n_estimators=1000, subsample=0.9; total time= 1.8min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=5, n_estimators=1000, subsample=0.9; total time= 1.7min\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=1000, subsample=0.7; total time= 1.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=3, n_estimators=500, subsample=1.0; total time=  40.4s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, n_estimators=100, subsample=0.9; total time=  24.4s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9; total time=  20.2s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9; total time=  20.3s\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=  30.9s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  32.6s\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.4min\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=500, subsample=0.7; total time= 1.1min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.8; total time= 1.2min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7; total time=  25.8s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7; total time=  25.1s\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=5, n_estimators=1000, subsample=0.9; total time= 1.8min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=5, n_estimators=1000, subsample=0.9; total time= 1.7min\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=1000, subsample=0.7; total time= 1.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=3, n_estimators=500, subsample=1.0; total time=  41.2s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, n_estimators=100, subsample=0.9; total time=  24.3s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9; total time=  19.9s\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=  30.5s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  32.7s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  32.9s\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.4min\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=500, subsample=0.7; total time= 1.1min\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=500, subsample=0.7; total time= 1.1min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.8; total time= 1.1min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7; total time=  25.9s\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7; total time=  25.1s\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.4min\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=500, subsample=0.7; total time= 1.1min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.8; total time= 1.2min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.8; total time= 1.2min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=7, n_estimators=1000, subsample=0.8; total time= 1.6min\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=5,\n                   estimator=XGBClassifier(base_score=None, booster=None,\n                                           callbacks=None,\n                                           colsample_bylevel=None,\n                                           colsample_bynode=None,\n                                           colsample_bytree=None, device=None,\n                                           early_stopping_rounds=None,\n                                           enable_categorical=False,\n                                           eval_metric='auc',\n                                           feature_types=None, gamma=None,\n                                           grow_policy=None,\n                                           importance_type=None,\n                                           interaction_constraints=None,\n                                           learning_rat...\n                                           monotone_constraints=None,\n                                           multi_strategy=None,\n                                           n_estimators=None, n_jobs=None,\n                                           num_parallel_tree=None,\n                                           random_state=42, ...),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'colsample_bytree': [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n                                        'max_depth': [3, 5, 7, 10],\n                                        'n_estimators': [100, 200, 500, 1000],\n                                        'subsample': [0.7, 0.8, 0.9, 1.0]},\n                   random_state=42, scoring='roc_auc', verbose=2)","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=XGBClassifier(base_score=None, booster=None,\n                                           callbacks=None,\n                                           colsample_bylevel=None,\n                                           colsample_bynode=None,\n                                           colsample_bytree=None, device=None,\n                                           early_stopping_rounds=None,\n                                           enable_categorical=False,\n                                           eval_metric=&#x27;auc&#x27;,\n                                           feature_types=None, gamma=None,\n                                           grow_policy=None,\n                                           importance_type=None,\n                                           interaction_constraints=None,\n                                           learning_rat...\n                                           monotone_constraints=None,\n                                           multi_strategy=None,\n                                           n_estimators=None, n_jobs=None,\n                                           num_parallel_tree=None,\n                                           random_state=42, ...),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.2],\n                                        &#x27;max_depth&#x27;: [3, 5, 7, 10],\n                                        &#x27;n_estimators&#x27;: [100, 200, 500, 1000],\n                                        &#x27;subsample&#x27;: [0.7, 0.8, 0.9, 1.0]},\n                   random_state=42, scoring=&#x27;roc_auc&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=XGBClassifier(base_score=None, booster=None,\n                                           callbacks=None,\n                                           colsample_bylevel=None,\n                                           colsample_bynode=None,\n                                           colsample_bytree=None, device=None,\n                                           early_stopping_rounds=None,\n                                           enable_categorical=False,\n                                           eval_metric=&#x27;auc&#x27;,\n                                           feature_types=None, gamma=None,\n                                           grow_policy=None,\n                                           importance_type=None,\n                                           interaction_constraints=None,\n                                           learning_rat...\n                                           monotone_constraints=None,\n                                           multi_strategy=None,\n                                           n_estimators=None, n_jobs=None,\n                                           num_parallel_tree=None,\n                                           random_state=42, ...),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.2],\n                                        &#x27;max_depth&#x27;: [3, 5, 7, 10],\n                                        &#x27;n_estimators&#x27;: [100, 200, 500, 1000],\n                                        &#x27;subsample&#x27;: [0.7, 0.8, 0.9, 1.0]},\n                   random_state=42, scoring=&#x27;roc_auc&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"xgb_model_1.score(x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:43:02.806478Z","iopub.execute_input":"2025-08-25T13:43:02.806978Z","iopub.status.idle":"2025-08-25T13:43:05.156461Z","shell.execute_reply.started":"2025-08-25T13:43:02.806949Z","shell.execute_reply":"2025-08-25T13:43:05.155615Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"0.9667270282352419"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"xgb_model_1.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:43:48.790060Z","iopub.execute_input":"2025-08-25T13:43:48.790378Z","iopub.status.idle":"2025-08-25T13:43:48.797705Z","shell.execute_reply.started":"2025-08-25T13:43:48.790346Z","shell.execute_reply":"2025-08-25T13:43:48.796650Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"{'subsample': 0.8,\n 'n_estimators': 1000,\n 'max_depth': 7,\n 'learning_rate': 0.05,\n 'colsample_bytree': 0.9}"},"metadata":{}},{"name":"stdout","text":"[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=5, n_estimators=1000, subsample=0.9; total time= 1.8min\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=1000, subsample=0.7; total time= 1.4min\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=1000, subsample=0.7; total time= 1.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=3, n_estimators=500, subsample=1.0; total time=  40.7s\n[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=10, n_estimators=100, subsample=0.9; total time=  24.9s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.9; total time=  20.4s\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=  30.8s\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=  33.0s\n[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=7, n_estimators=200, subsample=0.8; total time=  32.6s\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.1, max_depth=3, n_estimators=1000, subsample=0.8; total time= 1.4min\n[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=500, subsample=0.7; total time= 1.1min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=500, subsample=0.8; total time= 1.2min\n[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.7; total time=  26.3s\n[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.4min\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# A small, fast grid for fine-tuning with GridSearchCV\nfast_grid = {\n    # Fine-tune the learning rate around the best value found\n    'learning_rate': [0.04, 0.05, 0.06],\n\n    # Fine-tune the max_depth around the best value found\n    'max_depth': [6, 7, 8],\n\n    # Keep the other parameters fixed at their best known values\n    'n_estimators': [1000],\n    'subsample': [0.8],\n    'colsample_bytree': [0.9]\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T14:01:09.394091Z","iopub.execute_input":"2025-08-25T14:01:09.394414Z","iopub.status.idle":"2025-08-25T14:01:09.400095Z","shell.execute_reply.started":"2025-08-25T14:01:09.394389Z","shell.execute_reply":"2025-08-25T14:01:09.399169Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"#lets do grid search cv\nxgb_grid_search_model = GridSearchCV(xgb,\n                                    param_grid = fast_grid,\n                                    n_jobs = -1,\n                                    verbose = 2,\n                                    scoring = 'roc_auc',\n                                    cv = 5)\nxgb_grid_search_model.fit(x_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T14:01:12.613191Z","iopub.execute_input":"2025-08-25T14:01:12.613572Z","iopub.status.idle":"2025-08-25T14:28:50.285690Z","shell.execute_reply.started":"2025-08-25T14:01:12.613532Z","shell.execute_reply":"2025-08-25T14:28:50.284480Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 9 candidates, totalling 45 fits\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.1min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.1min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.1min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.1min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.4min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.04, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=8, n_estimators=1000, subsample=0.8; total time= 1.7min\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False,\n                                     eval_metric='auc', feature_types=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None...\n                                     max_delta_step=None, max_depth=None,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=42, ...),\n             n_jobs=-1,\n             param_grid={'colsample_bytree': [0.9],\n                         'learning_rate': [0.04, 0.05, 0.06],\n                         'max_depth': [6, 7, 8], 'n_estimators': [1000],\n                         'subsample': [0.8]},\n             scoring='roc_auc', verbose=2)","text/html":"<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False,\n                                     eval_metric=&#x27;auc&#x27;, feature_types=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None...\n                                     max_delta_step=None, max_depth=None,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=42, ...),\n             n_jobs=-1,\n             param_grid={&#x27;colsample_bytree&#x27;: [0.9],\n                         &#x27;learning_rate&#x27;: [0.04, 0.05, 0.06],\n                         &#x27;max_depth&#x27;: [6, 7, 8], &#x27;n_estimators&#x27;: [1000],\n                         &#x27;subsample&#x27;: [0.8]},\n             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n             estimator=XGBClassifier(base_score=None, booster=None,\n                                     callbacks=None, colsample_bylevel=None,\n                                     colsample_bynode=None,\n                                     colsample_bytree=None, device=None,\n                                     early_stopping_rounds=None,\n                                     enable_categorical=False,\n                                     eval_metric=&#x27;auc&#x27;, feature_types=None,\n                                     gamma=None, grow_policy=None,\n                                     importance_type=None,\n                                     interaction_constraints=None,\n                                     learning_rate=None...\n                                     max_delta_step=None, max_depth=None,\n                                     max_leaves=None, min_child_weight=None,\n                                     missing=nan, monotone_constraints=None,\n                                     multi_strategy=None, n_estimators=None,\n                                     n_jobs=None, num_parallel_tree=None,\n                                     random_state=42, ...),\n             n_jobs=-1,\n             param_grid={&#x27;colsample_bytree&#x27;: [0.9],\n                         &#x27;learning_rate&#x27;: [0.04, 0.05, 0.06],\n                         &#x27;max_depth&#x27;: [6, 7, 8], &#x27;n_estimators&#x27;: [1000],\n                         &#x27;subsample&#x27;: [0.8]},\n             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"xgb_grid_search_model.score(x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T14:30:30.132000Z","iopub.execute_input":"2025-08-25T14:30:30.132307Z","iopub.status.idle":"2025-08-25T14:30:32.070351Z","shell.execute_reply.started":"2025-08-25T14:30:30.132288Z","shell.execute_reply":"2025-08-25T14:30:32.069025Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"0.9667428530907539"},"metadata":{}},{"name":"stdout","text":"[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.05, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.6min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=6, n_estimators=1000, subsample=0.8; total time= 2.0min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=7, n_estimators=1000, subsample=0.8; total time= 2.3min\n[CV] END colsample_bytree=0.9, learning_rate=0.06, max_depth=8, n_estimators=1000, subsample=0.8; total time= 2.5min\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"xgb_grid_search_model.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T14:31:56.945768Z","iopub.execute_input":"2025-08-25T14:31:56.946124Z","iopub.status.idle":"2025-08-25T14:31:56.952104Z","shell.execute_reply.started":"2025-08-25T14:31:56.946100Z","shell.execute_reply":"2025-08-25T14:31:56.951218Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"{'colsample_bytree': 0.9,\n 'learning_rate': 0.06,\n 'max_depth': 6,\n 'n_estimators': 1000,\n 'subsample': 0.8}"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"# lets hypertune lgbm now \nlgbm_model_1 = RandomizedSearchCV(lgbm,\n                                 param_distributions = lgbm_grid,\n                                 cv = 5,\n                                 scoring = \"roc_auc\",\n                                 n_iter = 20,\n                                 n_jobs = -1,\n                                 verbose = True,\n                                 random_state = 42)\nlgbm_model_1.fit(x_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T14:36:14.310470Z","iopub.execute_input":"2025-08-25T14:36:14.310827Z","iopub.status.idle":"2025-08-25T15:12:33.312418Z","shell.execute_reply.started":"2025-08-25T14:36:14.310801Z","shell.execute_reply":"2025-08-25T15:12:33.311349Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 20 candidates, totalling 100 fits\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259425 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112058 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106208 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106032 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269992 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.294867 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252075 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161383 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234030 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091577 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.235124 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102876 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.169400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245900 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115333 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.120784 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102495 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198796 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.249844 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.111073 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.274857 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.109746 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216010 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.124552 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.312576 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101571 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097684 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099273 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.108402 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286743 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107675 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.097240 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250376 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.177162 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.101678 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.180264 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.183895 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237013 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234087 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234586 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264409 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.192105 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178425 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.247905 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237419 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.178090 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.106974 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.278850 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164751 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305823 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.162590 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259826 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252694 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232850 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103980 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.123093 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304282 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273083 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119644 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228454 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257134 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.133970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"RandomizedSearchCV(cv=5,\n                   estimator=LGBMClassifier(is_unbalance=True, metric='auc',\n                                            objective='binary',\n                                            random_state=42),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={'colsample_bytree': [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n                                        'n_estimators': [100, 200, 500, 1000],\n                                        'num_leaves': [20, 31, 40, 50, 60],\n                                        'subsample': [0.7, 0.8, 0.9, 1.0]},\n                   random_state=42, scoring='roc_auc', verbose=True)","text/html":"<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;,\n                                            objective=&#x27;binary&#x27;,\n                                            random_state=42),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.2],\n                                        &#x27;n_estimators&#x27;: [100, 200, 500, 1000],\n                                        &#x27;num_leaves&#x27;: [20, 31, 40, 50, 60],\n                                        &#x27;subsample&#x27;: [0.7, 0.8, 0.9, 1.0]},\n                   random_state=42, scoring=&#x27;roc_auc&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n                   estimator=LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;,\n                                            objective=&#x27;binary&#x27;,\n                                            random_state=42),\n                   n_iter=20, n_jobs=-1,\n                   param_distributions={&#x27;colsample_bytree&#x27;: [0.7, 0.8, 0.9,\n                                                             1.0],\n                                        &#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.2],\n                                        &#x27;n_estimators&#x27;: [100, 200, 500, 1000],\n                                        &#x27;num_leaves&#x27;: [20, 31, 40, 50, 60],\n                                        &#x27;subsample&#x27;: [0.7, 0.8, 0.9, 1.0]},\n                   random_state=42, scoring=&#x27;roc_auc&#x27;, verbose=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;, objective=&#x27;binary&#x27;,\n               random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;, objective=&#x27;binary&#x27;,\n               random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgbm_model_1.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:13:42.088357Z","iopub.execute_input":"2025-08-25T15:13:42.088708Z","iopub.status.idle":"2025-08-25T15:13:42.096346Z","shell.execute_reply.started":"2025-08-25T15:13:42.088683Z","shell.execute_reply":"2025-08-25T15:13:42.094881Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'subsample': 0.8,\n 'num_leaves': 60,\n 'n_estimators': 1000,\n 'learning_rate': 0.05,\n 'colsample_bytree': 0.8}"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"lgbm_model_1.score(x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:13:45.997681Z","iopub.execute_input":"2025-08-25T15:13:45.997961Z","iopub.status.idle":"2025-08-25T15:13:54.955680Z","shell.execute_reply.started":"2025-08-25T15:13:45.997942Z","shell.execute_reply":"2025-08-25T15:13:54.955017Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"0.9673425194571471"},"metadata":{}},{"name":"stdout","text":"\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.129882 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287616 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091078 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.088408 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104484 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251118 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.247144 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152660 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121097 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256465 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.187730 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268319 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.100975 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.253990 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113369 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216749 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254256 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118351 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265119 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098049 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112913 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126972 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117931 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218865 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148276 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.148625 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304835 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.107738 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161012 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257737 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.151211 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229001 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185726 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# grid search cv on lgbm \nlgbm_fine_tune_grid = {\n    'num_leaves': [55, 60, 65],\n    'learning_rate': [0.04, 0.05, 0.06],\n    'n_estimators': [1000],\n    'subsample': [0.8],\n    'colsample_bytree': [0.8]\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:18:19.767528Z","iopub.execute_input":"2025-08-25T15:18:19.767896Z","iopub.status.idle":"2025-08-25T15:18:19.774204Z","shell.execute_reply.started":"2025-08-25T15:18:19.767860Z","shell.execute_reply":"2025-08-25T15:18:19.772351Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"lgbm_grid_search_model = GridSearchCV(estimator=lgbm,\n    param_grid=lgbm_fine_tune_grid,\n    scoring='roc_auc',\n    cv=5,\n    verbose=2,\n    n_jobs=-1\n)\nlgbm_grid_search_model.fit(x_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:19:49.156278Z","iopub.execute_input":"2025-08-25T15:19:49.156608Z","iopub.status.idle":"2025-08-25T15:50:22.112098Z","shell.execute_reply.started":"2025-08-25T15:19:49.156578Z","shell.execute_reply":"2025-08-25T15:50:22.111106Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 9 candidates, totalling 45 fits\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092777 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(cv=5,\n             estimator=LGBMClassifier(is_unbalance=True, metric='auc',\n                                      objective='binary', random_state=42),\n             n_jobs=-1,\n             param_grid={'colsample_bytree': [0.8],\n                         'learning_rate': [0.04, 0.05, 0.06],\n                         'n_estimators': [1000], 'num_leaves': [55, 60, 65],\n                         'subsample': [0.8]},\n             scoring='roc_auc', verbose=2)","text/html":"<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n             estimator=LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;,\n                                      objective=&#x27;binary&#x27;, random_state=42),\n             n_jobs=-1,\n             param_grid={&#x27;colsample_bytree&#x27;: [0.8],\n                         &#x27;learning_rate&#x27;: [0.04, 0.05, 0.06],\n                         &#x27;n_estimators&#x27;: [1000], &#x27;num_leaves&#x27;: [55, 60, 65],\n                         &#x27;subsample&#x27;: [0.8]},\n             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n             estimator=LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;,\n                                      objective=&#x27;binary&#x27;, random_state=42),\n             n_jobs=-1,\n             param_grid={&#x27;colsample_bytree&#x27;: [0.8],\n                         &#x27;learning_rate&#x27;: [0.04, 0.05, 0.06],\n                         &#x27;n_estimators&#x27;: [1000], &#x27;num_leaves&#x27;: [55, 60, 65],\n                         &#x27;subsample&#x27;: [0.8]},\n             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;, objective=&#x27;binary&#x27;,\n               random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;, objective=&#x27;binary&#x27;,\n               random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"lgbm_grid_search_model.score(x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T15:51:22.007460Z","iopub.execute_input":"2025-08-25T15:51:22.007879Z","iopub.status.idle":"2025-08-25T15:51:30.071916Z","shell.execute_reply.started":"2025-08-25T15:51:22.007852Z","shell.execute_reply":"2025-08-25T15:51:30.070909Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"0.9674099581032443"},"metadata":{}},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.324977 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.6min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.353051 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.5min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.096565 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.314385 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.6min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182244 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.3min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244075 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.4min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243317 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.6min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230894 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.9min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.255481 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.160529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.6min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233124 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.3min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234788 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.5min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.173277 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.3min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309046 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.9min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.314761 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.3min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262659 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.5min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241487 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.3min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233778 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.6min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.301153 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.2min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.240318 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.4min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243559 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.4min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254561 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.9min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117915 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.4min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284773 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.2min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258519 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.5min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214584 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.1min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302611 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.9min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232297 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.3min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114214 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.4min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103168 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.1min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.227130 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.8min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273035 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.312749 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 1.9min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133467 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.2min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256477 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.4min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.331014 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.8min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266231 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.5min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098583 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.2min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.164260 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.5min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228548 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.2min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098933 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.4min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 765\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.1min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.155509 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.8min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251979 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.3min\n[LightGBM] [Info] Number of positive: 57912, number of negative: 422088\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257429 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 480000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 1.4min\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"logistic_regression_base_model_score = log_reg.score(x_test,y_test)\nlogistic_regression_randomized_search_model_score = log_reg_1.score(x_test,y_test)\nlogistic_regression_grid_search_model_score = log_reg_grid.score(x_test,y_test)\n\nxgb_base_model_score = xgb.score(x_test,y_test)\nxgb_randomized_search_model_score = xgb_model_1.score(x_test,y_test)\nxgb_grid_search_model_score = xgb_grid_search_model.score(x_test,y_test)\n\nlgbm_base_model_score = lgbm.score(x_test,y_test)\nlgbm_randomized_search_model_score = lgbm_model_1.score(x_test,y_test)\nlgbm_grid_search_model_score = lgbm_grid_search_model.score(x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:04:58.116161Z","iopub.execute_input":"2025-08-25T16:04:58.116509Z","iopub.status.idle":"2025-08-25T16:04:58.338103Z","shell.execute_reply.started":"2025-08-25T16:04:58.116474Z","shell.execute_reply":"2025-08-25T16:04:58.336778Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3183534274.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogistic_regression_base_model_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogistic_regression_randomized_search_model_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogistic_regression_grid_search_model_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb_base_model_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'score'"],"ename":"AttributeError","evalue":"'dict' object has no attribute 'score'","output_type":"error"}],"execution_count":65},{"cell_type":"code","source":"# Logistic Regression Scores\nlogistic_regression_base_model_score = log_reg.score(x_test_scaled, y_test)\nlogistic_regression_randomized_search_model_score = log_reg_1.score(x_test, y_test)\nlogistic_regression_grid_search_model_score = log_reg_grid_model.score(x_test, y_test)\n\n# XGBoost Scores\nxgb_base_model_score = xgb.score(x_test, y_test)\nxgb_randomized_search_model_score = xgb_model_1.score(x_test, y_test)\nxgb_grid_search_model_score = xgb_grid_search_model.score(x_test, y_test)\n\n# LightGBM Scores\nlgbm_base_model_score = lgbm.score(x_test, y_test)\nlgbm_randomized_search_model_score = lgbm_model_1.score(x_test, y_test)\nlgbm_grid_search_model_score = lgbm_grid_search_model.score(x_test, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:06:14.675748Z","iopub.execute_input":"2025-08-25T16:06:14.676094Z","iopub.status.idle":"2025-08-25T16:06:36.966796Z","shell.execute_reply.started":"2025-08-25T16:06:14.676069Z","shell.execute_reply":"2025-08-25T16:06:36.965735Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n  warnings.warn(\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"print(\"--- Logistic Regression ---\")\nprint(f\"Base Model Score: {logistic_regression_base_model_score:.4f}\")\nprint(f\"RandomizedSearch Score: {logistic_regression_randomized_search_model_score:.4f}\")\nprint(f\"GridSearch Score: {logistic_regression_grid_search_model_score:.4f}\")\nprint(\"-\" * 30)\n\nprint(\"--- XGBoost ---\")\nprint(f\"Base Model Score: {xgb_base_model_score:.4f}\")\nprint(f\"RandomizedSearch Score: {xgb_randomized_search_model_score:.4f}\")\nprint(f\"GridSearch Score: {xgb_grid_search_model_score:.4f}\")\nprint(\"-\" * 30)\n\nprint(\"--- LightGBM ---\")\nprint(f\"Base Model Score: {lgbm_base_model_score:.4f}\")\nprint(f\"RandomizedSearch Score: {lgbm_randomized_search_model_score:.4f}\")\nprint(f\"GridSearch Score: {lgbm_grid_search_model_score:.4f}\")\nprint(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:09:47.936912Z","iopub.execute_input":"2025-08-25T16:09:47.937254Z","iopub.status.idle":"2025-08-25T16:09:47.945216Z","shell.execute_reply.started":"2025-08-25T16:09:47.937230Z","shell.execute_reply":"2025-08-25T16:09:47.944056Z"}},"outputs":[{"name":"stdout","text":"--- Logistic Regression ---\nBase Model Score: 0.8691\nRandomizedSearch Score: 0.9427\nGridSearch Score: 0.9427\n------------------------------\n--- XGBoost ---\nBase Model Score: 0.8860\nRandomizedSearch Score: 0.9667\nGridSearch Score: 0.9667\n------------------------------\n--- LightGBM ---\nBase Model Score: 0.8795\nRandomizedSearch Score: 0.9673\nGridSearch Score: 0.9674\n------------------------------\n","output_type":"stream"}],"execution_count":70},{"cell_type":"markdown","source":"### let's visualise what we have done so far ","metadata":{}},{"cell_type":"markdown","source":"#### Comparing All Model Scores","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Let's recreate the scores_df DataFrame for this example\nscores_data = {\n    'Base Model': [logistic_regression_base_model_score, xgb_base_model_score, lgbm_base_model_score],\n    'RandomizedSearch': [logistic_regression_randomized_search_model_score, xgb_randomized_search_model_score, lgbm_randomized_search_model_score],\n    'GridSearch': [logistic_regression_grid_search_model_score, xgb_grid_search_model_score, lgbm_grid_search_model_score]\n}\nscores_df = pd.DataFrame(scores_data, index=['Logistic Regression', 'XGBoost', 'LightGBM'])\n\n# We need to 'melt' the DataFrame to make it suitable for seaborn\nplot_df = scores_df.reset_index().melt(id_vars='index', var_name='Tuning Stage', value_name='ROC AUC')\nplot_df.rename(columns={'index': 'Model'}, inplace=True)\n\n# Create the bar plot\nplt.figure(figsize=(12, 7))\nsns.barplot(data=plot_df, x='Model', y='ROC AUC', hue='Tuning Stage', palette='viridis')\n\n# Add titles and labels for clarity\nplt.title('Model Performance Comparison (ROC AUC)', fontsize=16)\nplt.xlabel('Model', fontsize=12)\nplt.ylabel('ROC AUC Score', fontsize=12)\nplt.ylim(0.85, 0.98) # Zoom in on the relevant score range\nplt.xticks(rotation=15) # Rotate labels slightly\nplt.legend(title='Tuning Stage')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('model_comparison_scores.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:13:46.990512Z","iopub.execute_input":"2025-08-25T16:13:46.991217Z","iopub.status.idle":"2025-08-25T16:13:47.960242Z","shell.execute_reply.started":"2025-08-25T16:13:46.991190Z","shell.execute_reply":"2025-08-25T16:13:47.959235Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x700 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":71},{"cell_type":"markdown","source":"#### Feature Importance of the Winning Model\n","metadata":{}},{"cell_type":"code","source":"# The best model is your fitted GridSearchCV object for LightGBM\nbest_lgbm_model = lgbm_grid_search_model.best_estimator_\n\n# Get feature importances\nimportances = best_lgbm_model.feature_importances_\n\n# Get feature names (assuming your training data 'X' is a DataFrame)\nfeature_names = x.columns\n\n# Create a DataFrame for visualization\nfeature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n\n# Sort by importance and select the top 20 features\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).head(20)\n\n# Create the horizontal bar plot\nplt.figure(figsize=(10, 8))\nsns.barplot(data=feature_importance_df, x='Importance', y='Feature', palette='rocket')\n\n# Add titles and labels\nplt.title('Top 20 Feature Importances (Tuned LightGBM)', fontsize=16)\nplt.xlabel('Importance Score', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('feature_importance.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:15:03.174727Z","iopub.execute_input":"2025-08-25T16:15:03.175247Z","iopub.status.idle":"2025-08-25T16:15:03.751968Z","shell.execute_reply.started":"2025-08-25T16:15:03.175074Z","shell.execute_reply":"2025-08-25T16:15:03.750866Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x800 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":74},{"cell_type":"code","source":"# preprocessing the test csv ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:30:06.219884Z","iopub.execute_input":"2025-08-25T16:30:06.220264Z","iopub.status.idle":"2025-08-25T16:30:06.226106Z","shell.execute_reply.started":"2025-08-25T16:30:06.220237Z","shell.execute_reply":"2025-08-25T16:30:06.224506Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"test_csv.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:30:14.404960Z","iopub.execute_input":"2025-08-25T16:30:14.405299Z","iopub.status.idle":"2025-08-25T16:30:14.609984Z","shell.execute_reply.started":"2025-08-25T16:30:14.405275Z","shell.execute_reply":"2025-08-25T16:30:14.608666Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 250000 entries, 0 to 249999\nData columns (total 17 columns):\n #   Column     Non-Null Count   Dtype \n---  ------     --------------   ----- \n 0   id         250000 non-null  int64 \n 1   age        250000 non-null  int64 \n 2   job        250000 non-null  object\n 3   marital    250000 non-null  object\n 4   education  250000 non-null  object\n 5   default    250000 non-null  object\n 6   balance    250000 non-null  int64 \n 7   housing    250000 non-null  object\n 8   loan       250000 non-null  object\n 9   contact    250000 non-null  object\n 10  day        250000 non-null  int64 \n 11  month      250000 non-null  object\n 12  duration   250000 non-null  int64 \n 13  campaign   250000 non-null  int64 \n 14  pdays      250000 non-null  int64 \n 15  previous   250000 non-null  int64 \n 16  poutcome   250000 non-null  object\ndtypes: int64(8), object(9)\nmemory usage: 32.4+ MB\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"test_ids = test_csv['id']\nprint(\"Applying preprocessing steps to test_csv...\")\ntest_processed = test_csv.copy()\ntest_processed = test_processed.drop(\"marital\", axis=1)\ntest_processed = test_processed.drop(\"id\", axis=1)\nbinary_cols = [\"housing\", \"loan\", \"default\"]\nfor col in binary_cols:\n    test_processed[col] = test_processed[col].map({'yes': 1, 'no': 0})\ncategorical_variables = [\"job\", \"education\", \"contact\", \"month\", \"poutcome\"]\ntest_processed = pd.get_dummies(test_processed, columns=categorical_variables, drop_first=True, dtype=int)\ntest_processed['pcontacted'] = (test_processed['pdays'] != -1).astype(int)\ntest_processed = test_processed.drop(\"pdays\", axis=1)\n\nprint(\"Aligning columns with the training data...\")\ntest_processed = test_processed.reindex(columns=x.columns, fill_value=0)\nprint(\"\\nPreprocessing of test_csv is complete.\")\nprint(f\"Shape of your processed training data (x): {x.shape}\")\nprint(f\"Shape of your processed test data:      {test_processed.shape}\")\n\ndisplay(test_processed.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:33:08.876939Z","iopub.execute_input":"2025-08-25T16:33:08.877278Z","iopub.status.idle":"2025-08-25T16:33:09.651922Z","shell.execute_reply.started":"2025-08-25T16:33:08.877256Z","shell.execute_reply":"2025-08-25T16:33:09.650768Z"}},"outputs":[{"name":"stdout","text":"Applying preprocessing steps to test_csv...\nAligning columns with the training data...\n\nPreprocessing of test_csv is complete.\nShape of your processed training data (x): (750000, 40)\nShape of your processed test data:      (250000, 40)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   age  default  balance  housing  loan  day  duration  campaign  previous  \\\n0   32        0     1397        1     0   21       224         1         0   \n1   44        0       23        1     0    3       586         2         0   \n2   36        0       46        1     1   13       111         2         0   \n3   58        0    -1380        1     1   29       125         1         0   \n4   28        0     1950        1     0   22       181         1         0   \n\n   job_blue-collar  ...  month_jun  month_mar  month_may  month_nov  \\\n0                1  ...          0          0          1          0   \n1                0  ...          0          0          0          0   \n2                0  ...          0          0          1          0   \n3                1  ...          0          0          1          0   \n4                0  ...          0          0          0          0   \n\n   month_oct  month_sep  poutcome_other  poutcome_success  poutcome_unknown  \\\n0          0          0               0                 0                 1   \n1          0          0               0                 0                 1   \n2          0          0               0                 0                 1   \n3          0          0               0                 0                 1   \n4          0          0               0                 0                 1   \n\n   pcontacted  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n\n[5 rows x 40 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>default</th>\n      <th>balance</th>\n      <th>housing</th>\n      <th>loan</th>\n      <th>day</th>\n      <th>duration</th>\n      <th>campaign</th>\n      <th>previous</th>\n      <th>job_blue-collar</th>\n      <th>...</th>\n      <th>month_jun</th>\n      <th>month_mar</th>\n      <th>month_may</th>\n      <th>month_nov</th>\n      <th>month_oct</th>\n      <th>month_sep</th>\n      <th>poutcome_other</th>\n      <th>poutcome_success</th>\n      <th>poutcome_unknown</th>\n      <th>pcontacted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>0</td>\n      <td>1397</td>\n      <td>1</td>\n      <td>0</td>\n      <td>21</td>\n      <td>224</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44</td>\n      <td>0</td>\n      <td>23</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>586</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36</td>\n      <td>0</td>\n      <td>46</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13</td>\n      <td>111</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58</td>\n      <td>0</td>\n      <td>-1380</td>\n      <td>1</td>\n      <td>1</td>\n      <td>29</td>\n      <td>125</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>0</td>\n      <td>1950</td>\n      <td>1</td>\n      <td>0</td>\n      <td>22</td>\n      <td>181</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 40 columns</p>\n</div>"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"lgbm_grid_search_model.fit(x, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T16:38:39.701399Z","iopub.execute_input":"2025-08-25T16:38:39.701854Z","iopub.status.idle":"2025-08-25T17:16:05.834043Z","shell.execute_reply.started":"2025-08-25T16:38:39.701809Z","shell.execute_reply":"2025-08-25T17:16:05.832826Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 9 candidates, totalling 45 fits\n[LightGBM] [Info] Number of positive: 90488, number of negative: 659512\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124861 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 770\n[LightGBM] [Info] Number of data points in the train set: 750000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120651 -> initscore=-1.986283\n[LightGBM] [Info] Start training from score -1.986283\n","output_type":"stream"},{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(cv=5,\n             estimator=LGBMClassifier(is_unbalance=True, metric='auc',\n                                      objective='binary', random_state=42),\n             n_jobs=-1,\n             param_grid={'colsample_bytree': [0.8],\n                         'learning_rate': [0.04, 0.05, 0.06],\n                         'n_estimators': [1000], 'num_leaves': [55, 60, 65],\n                         'subsample': [0.8]},\n             scoring='roc_auc', verbose=2)","text/html":"<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n             estimator=LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;,\n                                      objective=&#x27;binary&#x27;, random_state=42),\n             n_jobs=-1,\n             param_grid={&#x27;colsample_bytree&#x27;: [0.8],\n                         &#x27;learning_rate&#x27;: [0.04, 0.05, 0.06],\n                         &#x27;n_estimators&#x27;: [1000], &#x27;num_leaves&#x27;: [55, 60, 65],\n                         &#x27;subsample&#x27;: [0.8]},\n             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n             estimator=LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;,\n                                      objective=&#x27;binary&#x27;, random_state=42),\n             n_jobs=-1,\n             param_grid={&#x27;colsample_bytree&#x27;: [0.8],\n                         &#x27;learning_rate&#x27;: [0.04, 0.05, 0.06],\n                         &#x27;n_estimators&#x27;: [1000], &#x27;num_leaves&#x27;: [55, 60, 65],\n                         &#x27;subsample&#x27;: [0.8]},\n             scoring=&#x27;roc_auc&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;, objective=&#x27;binary&#x27;,\n               random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(is_unbalance=True, metric=&#x27;auc&#x27;, objective=&#x27;binary&#x27;,\n               random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.388362 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.5min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102638 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.8min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.522204 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.7min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213373 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.5min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207366 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.1min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289432 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.5min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.355305 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.3min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.399786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.3min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213048 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.9min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.328272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.2min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126826 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.7min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.416299 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.426366 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.7min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.199059 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.3min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.298425 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.7min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.313513 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.6min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.281800 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.3min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282060 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.5min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309156 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.1min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.9min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297703 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.1min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.335785 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.482930 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.386255 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.4min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.412525 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.7min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.128622 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.6min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295506 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.1min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.345700 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.8min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.329594 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.8min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289699 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.7min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.303496 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.8min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239303 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.4min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252872 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.1min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186024 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.1min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.171539 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.326851 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.3min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302874 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.04, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.6min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.137327 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 2.9min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.325845 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 3.9min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.322328 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 766\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.05, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 3.4min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.150502 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 767\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=55, subsample=0.8; total time= 3.0min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.131774 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 769\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=60, subsample=0.8; total time= 2.8min\n[LightGBM] [Info] Number of positive: 72391, number of negative: 527609\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125251 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 771\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120652 -> initscore=-1.986273\n[LightGBM] [Info] Start training from score -1.986273\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 2.7min\n[LightGBM] [Info] Number of positive: 72390, number of negative: 527610\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.332535 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 768\n[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 40\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120650 -> initscore=-1.986289\n[LightGBM] [Info] Start training from score -1.986289\n[CV] END colsample_bytree=0.8, learning_rate=0.06, n_estimators=1000, num_leaves=65, subsample=0.8; total time= 1.9min\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"print(\"Making final predictions on the test data...\")\n\ntest_predictions = lgbm_grid_search_model.predict_proba(test_processed)[:, 1]\n\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'y': test_predictions\n})\n\n# Save the file in the correct format\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nSubmission file 'submission.csv' has been created successfully!\")\ndisplay(submission_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:34:04.714891Z","iopub.execute_input":"2025-08-25T17:34:04.715273Z","iopub.status.idle":"2025-08-25T17:34:19.545870Z","shell.execute_reply.started":"2025-08-25T17:34:04.715242Z","shell.execute_reply":"2025-08-25T17:34:19.544852Z"}},"outputs":[{"name":"stdout","text":"Making final predictions on the test data...\n\nSubmission file 'submission.csv' has been created successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       id         y\n0  750000  0.010960\n1  750001  0.545076\n2  750002  0.000570\n3  750003  0.000113\n4  750004  0.105321","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>750000</td>\n      <td>0.010960</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>750001</td>\n      <td>0.545076</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>750002</td>\n      <td>0.000570</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>750003</td>\n      <td>0.000113</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>750004</td>\n      <td>0.105321</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"# 1. Get predictions from your best tuned XGBoost model\nprint(\"Getting predictions from XGBoost...\")\nxgb_preds = xgb_grid_search_model.predict_proba(test_processed)[:, 1]\n\n# 2. Get predictions from your best tuned LightGBM model\nprint(\"Getting predictions from LightGBM...\")\nlgbm_preds = lgbm_grid_search_model.predict_proba(test_processed)[:, 1]\n\n# 3. Average the two sets of predictions\nprint(\"Averaging predictions...\")\nensemble_preds = (xgb_preds + lgbm_preds) / 2\n\n# 4. Create the new submission DataFrame\nsubmission_df_ensemble = pd.DataFrame({\n    'id': test_ids,\n    'y': ensemble_preds\n})\n\n# 5. Save the new submission file\nsubmission_df_ensemble.to_csv('submission_ensemble.csv', index=False)\n\nprint(\"\\nNew ensemble submission file 'submission_ensemble.csv' created successfully!\")\ndisplay(submission_df_ensemble.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:49:12.262510Z","iopub.execute_input":"2025-08-25T17:49:12.262926Z","iopub.status.idle":"2025-08-25T17:49:29.411090Z","shell.execute_reply.started":"2025-08-25T17:49:12.262889Z","shell.execute_reply":"2025-08-25T17:49:29.409419Z"}},"outputs":[{"name":"stdout","text":"Getting predictions from XGBoost...\nGetting predictions from LightGBM...\nAveraging predictions...\n\nNew ensemble submission file 'submission_ensemble.csv' created successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       id         y\n0  750000  0.008728\n1  750001  0.545585\n2  750002  0.000687\n3  750003  0.000099\n4  750004  0.083114","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>750000</td>\n      <td>0.008728</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>750001</td>\n      <td>0.545585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>750002</td>\n      <td>0.000687</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>750003</td>\n      <td>0.000099</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>750004</td>\n      <td>0.083114</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":80},{"cell_type":"markdown","source":"## Final Summary & Conclusion\nThis project was a comprehensive, end-to-end machine learning competition focused on predicting bank term deposit subscriptions. The entire workflow, from initial data exploration to advanced model tuning and final submission, was successfully executed.\n\n`Methodology`\n\nThe approach followed a structured, professional workflow:\n\n`Exploratory Data Analysis (EDA)`: The dataset was thoroughly analyzed to understand feature distributions, correlations, and the significant class imbalance in the target variable.\n\n`Preprocessing`: A robust preprocessing pipeline was established to handle categorical and numerical data, including one-hot encoding and the creation of custom features.\n\n`Modeling & Tuning`: Several models were evaluated, with LightGBM and XGBoost showing the most promise. A two-step \"coarse-to-fine\" hyperparameter tuning strategy was employed, using RandomizedSearchCV for a broad search, followed by a focused GridSearchCV to fine-tune the best-performing models. The entire process was optimized for the competition's official ROC AUC metric.\n\n`Results & Key Learnings`\n\nThe final tuned LightGBM model achieved a public leaderboard score of 0.96864. This represents a substantial improvement over the initial baseline models, which scored around 0.88, demonstrating the immense value of rigorous hyperparameter tuning.\n\n`Key learnings from this project include`:\n\nThe critical impact of hyperparameter tuning on model performance.\n\nThe superior predictive power of gradient boosting models (LGBM, XGBoost) on structured, tabular data.\n\nPractical experience with the \"performance plateau,\" where the final, marginal gains are often achieved through advanced techniques like ensembling and deep feature engineering.\n\nUltimately, a highly competitive and robust predictive model was successfully developed, showcasing a strong command of the complete machine learning pipeline. 🏆","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}